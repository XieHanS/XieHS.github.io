<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://xiehans.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://xiehans.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-服务器环境搭建" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/04/21/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" class="article-date">
  <time datetime="2023-04-21T05:36:01.087Z" itemprop="datePublished">2023-04-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="服务器环境搭建"><a href="#服务器环境搭建" class="headerlink" title="服务器环境搭建"></a>服务器环境搭建</h1><p>1、安装docker阿里云镜像</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://xiehans.github.io/2023/04/21/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" data-id="clgt0zr32000musu10a2zf6yo" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/04/21/hello-world/" class="article-date">
  <time datetime="2023-04-21T03:44:11.011Z" itemprop="datePublished">2023-04-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/04/21/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My NewPost&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://xiehans.github.io/2023/04/21/hello-world/" data-id="clgt0zr2w000cusu1bug0asgp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-MP算法解释" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/05/26/MP%E7%AE%97%E6%B3%95%E8%A7%A3%E9%87%8A/" class="article-date">
  <time datetime="2022-05-26T12:39:51.000Z" itemprop="datePublished">2022-05-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/05/26/MP%E7%AE%97%E6%B3%95%E8%A7%A3%E9%87%8A/">MP算法解释</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="利用MP算法实现信号的稀疏表示"><a href="#利用MP算法实现信号的稀疏表示" class="headerlink" title="利用MP算法实现信号的稀疏表示"></a>利用MP算法实现信号的稀疏表示</h1><p>信号压缩的步骤：</p>
<ul>
<li>1） 构造超完备字典A</li>
<li>2） 通过给定的信号y和超完备字典A，求取信号的稀疏表示x</li>
<li>3） 保存超完备字典A和稀疏信号x，由于x的size远远小于y，相当于实现了信号的压缩</li>
<li>4） 待信号需要显示或者处理时，通过A和x的矩阵相乘获取所需要的信号y</li>
</ul>
<p>信号的稀疏表示采用MP算法，步骤如下：</p>
<p>假如给定信号y&#x3D;(1.65,-0.25)和超完备字典A&#x3D;(d1,d2,d3),d1，d2，d3分别为超完备字典中的原子，且d1&#x3D;(-0.707;0.707)，d2&#x3D;(0.8;0.6)，d3&#x3D;(0;-1);</p>
<p>MP步骤：</p>
<p>求出给定信号y和超完备字典A中所有原子的内积<d1y>，也就是y在每个原子上面的投影，求出内积的绝对值最大所在的原子。</p>
<p>|<d1y>|&#x3D; |(-0.707;0.707) (1.65,-0.25) |&#x3D; |(-0.707)<em>1.65+0.707</em>(-0.25)| &#x3D; |-1.34| &#x3D; 1.34;</p>
<p>|<d2y>| &#x3D; |(0.8;0.6) (1.65,-0.25)| &#x3D; |0.8<em>1.65+0.6</em>(-0.25)| &#x3D; 1.17;</p>
<p>|<d3y>| &#x3D; |(0;-1) (1.65,-0.25)| &#x3D; |0<em>1.65+(-1)</em>(-0.25)| &#x3D; 0.25;</p>
<p>最大内积为d1，所以y在d1上面的投影值最大。看下图：</p>
<p><img src="/MP%E7%AE%97%E6%B3%95%E8%A7%A3%E9%87%8A/clip_image002.jpg" alt="img"></p>
<p>有图很清晰看到y在d1上面的投影的值最大，y和投影值在该轴上面的向量的差值，得到除去分量d1上面的分量之后的冗余r0向量，也即</p>
<p>r0 &#x3D; y-<d1y>*d1 &#x3D; （0.7;0.7）；</p>
<p>【小贴士：为什么不是r0 &#x3D; y-<d1y>？为什么要乘以d1？理由：内积是一个值，该值在对应坐标轴上面的向量需要乘以坐标轴。比如5在x轴上面的向量就是5*（1,0）&#x3D;(5,0),也即向量(5,0) 】</p>
<p>将图一中的residue 0向下移，使得向量起始位置和原点重合，如图二黑色线</p>
<p><img src="/MP%E7%AE%97%E6%B3%95%E8%A7%A3%E9%87%8A/clip_image004.jpg" alt="img"></p>
<p>继续求取冗余部分和其他原子的内积</p>
<p>【小贴士：为什么不是所有原子而是剩下原子？因为原信号已经在d1上面求取过分量了，不需要再求，另外，d1和r0正交，内积为0】</p>
<p>|&lt;r0, d2&gt;|&#x3D;0.98;</p>
<p>|&lt;r0, d3&gt;|&#x3D;0.70;</p>
<p>r0在d2上面的投影略大，因此选择在d2上的投影值，那么剩下的冗余就为：</p>
<p>r1 &#x3D; r0-&lt;r0, d2&gt;*d2 &#x3D; (-0.08;0.11)</p>
<p>如果给定的阈值为(0.2;0.2),则r1已经满足给定的阈值，可以结束了，</p>
<p>如果阈值较小，则继续。</p>
<p>将图二中的residue 1向上移动，使得向量的起始和圆点重合。如图三</p>
<p><img src="/MP%E7%AE%97%E6%B3%95%E8%A7%A3%E9%87%8A/clip_image006.jpg" alt="img"></p>
<p>&lt;r1, d3&gt;&#x3D; -0.08<em>0+0.11</em>(-1) &#x3D; -0.11;</p>
<p>r2 &#x3D; r1 – (-0.11)*(0,-1)&#x3D;(-0.08;0.11)-(0;0.11)&#x3D;(-0.08;0);</p>
<p>x &#x3D; (-1.34; 0.98; -0.11) 就是求得的稀疏表示。</p>
<p> 验证一下：</p>
<p>y&#96; &#x3D; A*x&#x3D;(d1, d2,d3)<em>x &#x3D; (-0.707, 0.8, 0; 0.707, 0.6, -1)</em>(-1.334;0.98;-0.11) &#x3D; (1.73;-0.25)</p>
<p>实际的y&#x3D;(1.65,-0.25);   r2刚好等于y-y&#96;&#x3D;(-0.08;0)</p>
<p>如果选择原子d1和d3，则没有被选择的原子d2对应的位置需要补0，x&#x3D;（-1.334; 0; -0.11）</p>
<p>【小贴士：可能会觉得这种稀疏表示貌似不能应用于压缩，因为稀疏表示后的结果x比原本的y要多，再加上要存储字典A，明显存储了多很多的数据。</p>
<p>解释：字典A的行数n一般等于y的数据长度，y只能为一维数据，二维数据需要切片为一行一行的数据；字典A的列数K是字典原子的数量，也就是基的数量，通常情况K的值要远远大于n；如果每次迭代都是将所有原子都包含，那么最后得到的x的稀疏矩阵的长度就是原子的个数K，那这个x的size就远远大于y。而在实际问题中，往往前面几个原子就能表达原始数据90%以上的信息，因此只需要保留前面m个值即可。另外，迭代的时候通常也不需要迭代K次结束，而是达到一个可接受的冗余即可】</p>
<p>-—————————————————————————–</p>
<p><strong>例子</strong>：字典A是10<em>100，也就是有100个原子，y是一幅1000</em>10的图片，如果x中只有两个原子已经可以还原90%的数据信息，那么最后的稀疏矩阵就是1000*2。</p>
<p>这样，原本存储的数据量为1000<em>10&#x3D;10000，现在存储10</em>100+1000*2&#x3D;3000；</p>
<p>上面的解释是错误的，因为x只是数据y的稀疏表示，没有被选的原子在x需要用0替代，而非删除。</p>
<p><img src="/MP%E7%AE%97%E6%B3%95%E8%A7%A3%E9%87%8A/clip_image008.gif" alt="img"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://xiehans.github.io/2022/05/26/MP%E7%AE%97%E6%B3%95%E8%A7%A3%E9%87%8A/" data-id="clgt0zr2s0005usu1az2r9rrc" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-git常用命令" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/05/26/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" class="article-date">
  <time datetime="2022-05-26T12:25:56.000Z" itemprop="datePublished">2022-05-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/05/26/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">git常用命令</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1、	git diff &lt;name&gt; 查看文件的修改</span><br><span class="line">2、	git log --pretty=online 查看当前提交的日志</span><br><span class="line">3、	git reset --hard HEAD^ 回退到上一个版本</span><br><span class="line">4、	git reset --hard &lt;1094a&gt; 回退到制定版本</span><br><span class="line">5、	git reflog 查看每次命令的commit id</span><br><span class="line">6、	git checkout -- &lt;file name&gt; 把文件在工作区的修改全部撤销，回到暂存区状态（消除 add . 操作）</span><br><span class="line">7、	git reset HEAD &lt;file name&gt; 把暂存区的修改撤销，重新回到工作区（消除commit操作，执行上一条命令，可全部撤销）</span><br><span class="line">8、	git branch 查看多有分支</span><br><span class="line">9、	git checkout -b &lt;name&gt; 创建并切换分支</span><br><span class="line">10、	git checkout &lt;branch name&gt; 切换分支</span><br><span class="line">11、	git branch -d &lt;branch name&gt; 合并之后删除分支</span><br><span class="line">12、	git branch -D &lt;branch name&gt; 未合并之前强制删除分支</span><br><span class="line">13、	git merge 合并分支，log下面不保留合并分支状态</span><br><span class="line">14、	git merge --no-ff -m &quot;merge with no-ff&quot; dev 合并保留log下分支合并的状态</span><br><span class="line">15、	git stash 储藏当前工作状态</span><br><span class="line">16、	git stash list 查看工作现场</span><br><span class="line">17、	git stash pop 恢复当前工作状态，并删除stash的内容</span><br><span class="line">18、	git stash apply 恢复当前工作状态，不删除stash内容</span><br><span class="line">19、	git stash drop 上一条命令执行完，已恢复到当前工作状态后，删除stash的内容</span><br><span class="line">20、	git cherry-pick &lt;commit id&gt; 将commit id修改的内容复制到当前分支</span><br><span class="line">21、	git checkout -b dev origin/dev 创建远程origin的dev分支到本地</span><br><span class="line">22、	git push origin dev 推送到远程dev分支</span><br><span class="line">23、	git branch --set -upstream-to &lt;branch name&gt; origin/&lt;branch name&gt; 建立本地分支和远程分支的链接</span><br><span class="line">24、	git log --graph --pretty=oneline  --abbrev-commit 查看提交流图</span><br><span class="line">25、	git tag &lt;tag name&gt; &lt;commit id&gt; 给commit id这次提交打标签，标签和commit id 挂钩</span><br><span class="line">26、	git config --global alias.&quot;别名&quot; &quot;原命令&quot;，eg:git config --global alias.last &#x27;log -l&#x27;</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://xiehans.github.io/2022/05/26/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" data-id="clgt0zr2v0009usu1d9zhc00s" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" rel="tag">服务器</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-三步学会pytorch搭建深度模型" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/05/26/%E4%B8%89%E6%AD%A5%E5%AD%A6%E4%BC%9Apytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2022-05-26T12:07:19.000Z" itemprop="datePublished">2022-05-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/05/26/%E4%B8%89%E6%AD%A5%E5%AD%A6%E4%BC%9Apytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/">三步学会pytorch搭建深度模型</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文从最简单的卷积神经网络开始了解pytorch的框架。以下涉及到的代码完整版请查看git<a target="_blank" rel="noopener" href="https://github.com/XieHanS/CPSC_ECGHbClassify_demo.git">完整代码</a></p>
<p>基于pytorch的DL主要分为三个模块，数据块，模型块，和训练块。具体如下：</p>
<h1 id="1、-数据集"><a href="#1、-数据集" class="headerlink" title="1、   数据集"></a>1、   数据集</h1><p>pytorch提供有专门的数据下载，数据处理包，使用这些包可以极大地提高开发效率</p>
<h2 id="1-1-torch-utils-data工具包"><a href="#1-1-torch-utils-data工具包" class="headerlink" title="1.1 torch.utils.data工具包"></a>1.1 torch.utils.data工具包</h2><p>该包中有两个常用的类，分别为<strong>Dataset</strong>和<strong>DataLoader</strong></p>
<p>Dataset 一个抽象类，抽象类只能作为基类派生新类使用，不能创建抽象类对象。其他数据集需要继承这个类，并覆写其中的两个方法:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__getitem__ 和 __len__</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, label</span>):</span><br><span class="line">  	self.data = data</span><br><span class="line">    self.label = label</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">  	<span class="keyword">return</span> (torch.tensor(self.data[index], dtype=torch.<span class="built_in">float</span>), torch.tensor(self.label[index], dtype=torch.long))<span class="comment">#把numpy转换为Tensor</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br></pre></td></tr></table></figure>

<p>   调用 _ _ getitem_ _ 只返回一个样本，因此需要一个批量读取的工具，pytorch为我们提供了另外一个类DataLoader。DataLoader 定义一个新的迭代器，实现批量读取，打乱数据并提供并行加速等功能</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loader = torch.utils.data.DataLoader(</span><br><span class="line">  dataset = datasets,   <span class="comment">#加载的数据集</span></span><br><span class="line">  batch_size = BATCH_SIZE, <span class="comment">#批大小</span></span><br><span class="line">  shuffle = <span class="literal">True</span>,     <span class="comment">#是否将数据打乱，默认为false</span></span><br><span class="line">  num_workers=<span class="number">2</span>,<span class="comment">#使用多进程加载的进程数，0代表不使用多进程</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>使用的时候只需要调用这两个类即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#继承Dataset类，自定义数据集以及对应的标签</span></span><br><span class="line">dataset = MyDataset(X_train, Y_train)</span><br><span class="line">dataset_test = MyDataset(X_test, Y_test)</span><br><span class="line"><span class="comment">#装载数据，实现批量读取</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size)</span><br><span class="line">dataloader_test = DataLoader(dataset_test, batch_size=batch_size, drop_last=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#训练模型时候的数据循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epoch):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):  </span><br><span class="line">        input_x, input_y = <span class="built_in">tuple</span>(batch)</span><br><span class="line">        pred = model(input_x)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<h1 id="2、-定义模型"><a href="#2、-定义模型" class="headerlink" title="2、   定义模型"></a>2、   定义模型</h1><p>pytorch中自定义的模型往往继承自nn.Model，包括两个函数，初始化函数<code>__init__()</code>和forward()。<code>__init__()</code>定义了卷积层和其他层的参数，forward()规定了网络执行的顺序。例如下面的网络：</p>
<p><code>__init__()</code>定义了两个卷积层conv1和conv2，和一个全连接层out层，两个卷积层里面分别包含了一维卷积，DropOut，ReLU激活和最大池化层。</p>
<p>forward()定义了执行顺序。首先conv1，接着conv2，最后out层。由于上下层连接的问题，往往init里面会按照顺序撰写，而真正的执行顺序是forward里面的顺序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">    self.conv1 = nn.Sequential( <span class="comment"># input shape (1, 1, 2000)</span></span><br><span class="line">      nn.Conv1d(</span><br><span class="line">        in_channels=<span class="number">1</span>,     <span class="comment"># input height</span></span><br><span class="line">        out_channels=<span class="number">16</span>,    <span class="comment"># n_filters</span></span><br><span class="line">        kernel_size=<span class="number">5</span>,     <span class="comment"># filter size</span></span><br><span class="line">        stride=<span class="number">1</span>,        <span class="comment"># filter movement/step</span></span><br><span class="line">        padding=<span class="number">2</span>,</span><br><span class="line">      ),     <span class="comment"># output shape (16, 1, 2000)</span></span><br><span class="line">      nn.Dropout(<span class="number">0.2</span>),<span class="comment">#扔到0.2</span></span><br><span class="line">      nn.ReLU(),</span><br><span class="line">      nn.MaxPool1d(kernel_size=<span class="number">5</span>),  <span class="comment"># choose max value in 1x5 area, output shape (16, 1, 400)2000/5</span></span><br><span class="line">    )</span><br><span class="line">    self.conv2 = nn.Sequential( <span class="comment"># input shape (16, 1, 400)</span></span><br><span class="line">      nn.Conv1d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),<span class="comment"># output shape (32, 1, 400)</span></span><br><span class="line">      nn.Dropout(<span class="number">0.2</span>),<span class="comment">#扔掉0.2</span></span><br><span class="line">      nn.ReLU(),</span><br><span class="line">      nn.MaxPool1d(kernel_size=<span class="number">5</span>),<span class="comment"># output shape (32, 1, 400/5=80)</span></span><br><span class="line">    )</span><br><span class="line">    self.out = nn.Linear(<span class="number">32</span> * <span class="number">80</span>, <span class="number">3</span>)  <span class="comment"># fully connected layer, output 3 classes</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x = self.conv1(x)</span><br><span class="line">    x = self.conv2(x)</span><br><span class="line">    x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)     </span><br><span class="line">    output = self.out(x)</span><br><span class="line">    <span class="keyword">return</span> output, x </span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://www.processon.com/diagraming/5f8e903207912906db32acc2">https://www.processon.com/diagraming/5f8e903207912906db32acc2</a></p>
<p>我们打印出模型如下：</p>
<p><img src="/%E4%B8%89%E6%AD%A5%E5%AD%A6%E4%BC%9Apytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/image-20220526200852357.png" alt="image-20220526200852357"></p>
<h1 id="3、训练模型"><a href="#3、训练模型" class="headerlink" title="3、训练模型"></a>3、训练模型</h1><ul>
<li><p>1）读数据，装载数据，随机批量分配</p>
</li>
<li><p>2）初始化自定义的模型类</p>
</li>
<li><p>3）定义优化器【SGD，自适应优化算法(RMSProp，Adam，Adadelta)】和损失函数【MSE(回归),crossEntropy(分类)】</p>
</li>
<li><p>4）定义epoch数量，for循环训练模型</p>
<p># 初始化自定义的模型类</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">model = CNN()  </span><br><span class="line">model.verbose = <span class="literal">False</span><span class="comment">#运行的时候不显示详细信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器和损失函数</span></span><br><span class="line">LR = <span class="number">0.001</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=LR)  </span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()     </span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epoch):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):  </span><br><span class="line">      input_x, input_y = <span class="built_in">tuple</span>(batch)</span><br><span class="line">      pred = model(input_x)[<span class="number">0</span>]</span><br><span class="line">      loss = loss_func(pred, input_y)</span><br><span class="line">      optimizer.zero_grad()<span class="comment">#梯度置零</span></span><br><span class="line">      loss.backward()</span><br><span class="line">    <span class="comment">#optimizer.step()是大多数optimizer所支持的简化版本。一旦梯度被如backward()之类的函数计算好后，就可以调用这个函数更新所有的参数。</span></span><br><span class="line">      optimizer.step()</span><br><span class="line">      step += <span class="number">1</span></span><br><span class="line">	<span class="comment"># test</span></span><br><span class="line">	model.<span class="built_in">eval</span>()<span class="comment">#为了固定BN和dropout层,使得偏置参数不随着发生变化</span></span><br></pre></td></tr></table></figure>

<p>训练需要注意的几点：</p>
<ul>
<li>1）反向传播之前，优化器的梯度需要置零</li>
<li>2）optimizer.step()是大多数optimizer所支持的简化版本。一旦梯度被如backward()之类的函数计算好后，就可以调用这个函数更新所有的参数</li>
<li>3）model.eval()#训练完train_datasets之后,model要用来测试样本，model.eval()为了固定BN和dropout层,使得偏置参数不随着发生变化</li>
</ul>
<p>完整的代码<a target="_blank" rel="noopener" href="https://github.com/XieHanS/CPSC_ECGHbClassify_demo.git">https://github.com/XieHanS/CPSC_ECGHbClassify_demo.git</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://xiehans.github.io/2022/05/26/%E4%B8%89%E6%AD%A5%E5%AD%A6%E4%BC%9Apytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/" data-id="clgt0zr37000tusu1ffqc5p2q" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-重构带权重的深度模型" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/05/26/%E9%87%8D%E6%9E%84%E5%B8%A6%E6%9D%83%E9%87%8D%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2022-05-26T11:35:56.000Z" itemprop="datePublished">2022-05-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/05/26/%E9%87%8D%E6%9E%84%E5%B8%A6%E6%9D%83%E9%87%8D%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/">重构带权重的深度模型</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>1、导入文件</p>
<p>from keras.models import Model, load_model</p>
<p>1）导入带权重的.h&#x2F;.hdf5文件</p>
<p>model &#x3D; load_model(‘.h’);</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_arrhy = load_model(&#x27;V3.2-weights-463-0.996666.hdf5&#x27;)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>导入不带权重的json结构文件，和权重.h文件</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_qrs_cnn = model_from_json(open(&#x27;CNN.json&#x27;).read())</span><br><span class="line">model_qrs_cnn.load_weights(&#x27;CNN.h5&#x27;, by_name=True)</span><br></pre></td></tr></table></figure>

<p>2、打印出模型结构</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&#x27;模型结构为：&#x27;,model.summary())</span><br></pre></td></tr></table></figure>

<p>3、获取每层的配置变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config = model.get_config()</span><br></pre></td></tr></table></figure>

<p>config为字典类型，包括‘name’，‘layers’，‘input_layers’，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers = config[&#x27;layers&#x27;]</span><br></pre></td></tr></table></figure>

<p>查看每一层的详细配置信息<br><img src="/%E9%87%8D%E6%9E%84%E5%B8%A6%E6%9D%83%E9%87%8D%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/image-20220526194040757.png" alt="image-20220526194040757"></p>
<p>4、根据配置信息重构代码</p>
<p><img src="/%E9%87%8D%E6%9E%84%E5%B8%A6%E6%9D%83%E9%87%8D%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/image-20220526194104668.png" alt="image-20220526194104668"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv1d_1 = Conv1D(name=&#x27;conv1d_1&#x27;,filters=12,kernel_size=20,strides=1,padding=&#x27;same&#x27;,dilation_rate=1,activation=&#x27;linear&#x27;)(batch_normalization_1)</span><br></pre></td></tr></table></figure>

<p>5、最后一步</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = Model(inputs=[input_1],outputs=[output])</span><br></pre></td></tr></table></figure>

<p>6、将结构保存为json文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reconstructModel_json = model.to_json()</span><br><span class="line">with open(r&#x27;./reconstructModel.json&#x27;, &#x27;w&#x27;) as file:</span><br><span class="line">  file.write(reconstructModel_json) </span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://xiehans.github.io/2022/05/26/%E9%87%8D%E6%9E%84%E5%B8%A6%E6%9D%83%E9%87%8D%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/" data-id="clgt0zr38000uusu103ou1gub" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-IAR软件使用方法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/05/19/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/" class="article-date">
  <time datetime="2022-05-19T03:14:51.000Z" itemprop="datePublished">2022-05-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/05/19/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/">IAR软件使用方法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<h1 id="IAR-Embedded-Workbench-IDE-使用方法"><a href="#IAR-Embedded-Workbench-IDE-使用方法" class="headerlink" title="IAR Embedded Workbench IDE 使用方法"></a>IAR Embedded Workbench IDE 使用方法</h1><h2 id="0、软件安装"><a href="#0、软件安装" class="headerlink" title="0、软件安装"></a>0、软件安装</h2><p>网盘下载地址：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1edlsIZb4Pne13qAIx_y9dg?pwd=dkh8">https://pan.baidu.com/s/1edlsIZb4Pne13qAIx_y9dg?pwd=dkh8</a></p>
<p>提取码：dkh8</p>
<p>软件版本：7.80.4</p>
<p>注：内网位置：\192.168.2.58\EcgDoctor_data\开发工具\IAR for ARM 7.8.4.rar</p>
<p>下载后解压该压缩包，如下图：</p>
<p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220519134005237.png" alt="image-20220519134005237"></p>
<p>EWARM-CD-7804-12495.exe 为文件安装包，IARkg_Unis.exe 为激活工具。图片为激活步骤。</p>
<p>安装破解文字说明步骤如下：</p>
<p>1).下载IAR软件，然后点击安装，一路NEXT</p>
<p>2).安装后激活，需要断网</p>
<p>3).打开IAR软件，在软件目录中点击help&#x2F;License manger</p>
<p>4).弹出IAR License manager界面，点击license&#x2F;offline activation</p>
<p>5).出现license wizard界面</p>
<p>6).打开注册机，将license number 赋值到 license wizard界面的license number</p>
<p>7).在 license wizard界面点击“下一步”</p>
<p>8).点击”NO”</p>
<p>9).接下来会出现save activation information，选择一个地方存储activationinfo.txt文件</p>
<p>10).点击“下一步”</p>
<p>11).将第9步生成txt文件加载到注册机activate license中，点击activate license,会弹出另存生成一个activationresponse.txt文件</p>
<p>12).在第10步界面，use the response file to activate the license,中将第11步产生文件加载进来，点击“下一步”，完成注册</p>
<p>参考 <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_30480583/article/details/96544349">https://blog.csdn.net/weixin_30480583/article/details/96544349</a></p>
<h2 id="1、新建工作站"><a href="#1、新建工作站" class="headerlink" title="1、新建工作站"></a>1、新建工作站</h2><p>1）新建工作站</p>
<p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220519115357728.png" alt="image-20220519115357728"></p>
<p>2）、新建工程，Project-&gt; Creat New Project 选择  Externally built executable，在弹出的对话框中命名工程名称，保存到某个地址。</p>
<p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220519115616114.png" alt="image-20220519115616114"></p>
<p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220519115812003.png" alt="image-20220519115812003"></p>
<p>3）右键，Add -&gt; Add File 添加C&#x2F;C++文件</p>
<p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220519120206609.png" alt="image-20220519120206609"></p>
<p>4）添加完成，工作站呈下面样子</p>
<p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220519120313242.png" alt="image-20220519120313242"></p>
<h2 id="2、生成静态库文件"><a href="#2、生成静态库文件" class="headerlink" title="2、生成静态库文件"></a>2、生成静态库文件</h2><h3 id="2-1-选择输出library文件"><a href="#2-1-选择输出library文件" class="headerlink" title="2.1 选择输出library文件"></a>2.1 选择输出library文件</h3><p>右键-&gt;Options</p>
<p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220526192957199.png" alt="image-20220526192957199"></p>
<p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220729093722628.png" alt="image-20220729093722628"></p>
<p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220519113529754.png" alt="image-20220519113529754"></p>
<h3 id="2-2-选择编译语言"><a href="#2-2-选择编译语言" class="headerlink" title="2.2 选择编译语言"></a>2.2 选择编译语言</h3><p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220519113959652.png" alt="image-20220519113959652"></p>
<h3 id="2-3-修改将要生成的库文件名字"><a href="#2-3-修改将要生成的库文件名字" class="headerlink" title="2.3 修改将要生成的库文件名字"></a>2.3 修改将要生成的库文件名字</h3><p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220519113854258.png" alt="image-20220519113854258"></p>
<h3 id="2-4-修改为release模式，默认为debug模式"><a href="#2-4-修改为release模式，默认为debug模式" class="headerlink" title="2.4 修改为release模式，默认为debug模式"></a>2.4 修改为release模式，默认为debug模式</h3><p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220519115109182.png" alt="image-20220519115109182"></p>
<h3 id="2-5-生成库文件"><a href="#2-5-生成库文件" class="headerlink" title="2.5 生成库文件"></a>2.5 生成库文件</h3><p><img src="/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/image-20220526193159867.png" alt="image-20220526193159867"></p>
<h2 id="3、接口文件"><a href="#3、接口文件" class="headerlink" title="3、接口文件"></a>3、接口文件</h2><p>如果文件为.c后缀名，则可忽略下面操作。如果为C++，则需要添加</p>
<p>这里是在include头文件的外面包裹了<code>extern &quot;C&quot; &#123; &#125;</code>，是告诉编译器以C语言的命名方式去加载这个符号。还有一种比较常见的方式是在头文件中进行编译声明，如下所示，这样的话，无论C还是C++直接正常<code>include</code>就可以使用了。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __cplusplus</span></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> &#123;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">short</span> <span class="title function_">getresult</span><span class="params">(<span class="type">float</span> *sigSeg, <span class="type">short</span> segLen)</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __cplusplus</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://xiehans.github.io/2022/05/19/IAR%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/" data-id="clgt0zr2r0004usu14inq97jm" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%A1%AC%E4%BB%B6/" rel="tag">硬件</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-docker部署服务器" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/05/10/docker%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E5%99%A8/" class="article-date">
  <time datetime="2022-05-10T01:18:38.000Z" itemprop="datePublished">2022-05-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/05/10/docker%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E5%99%A8/">docker部署服务器</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install aliyun-python-sdk-core-v3</span><br><span class="line">pip install aliyun-python-sdk-sts </span><br></pre></td></tr></table></figure>

<h2 id="RabbitMQ"><a href="#RabbitMQ" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h2><p>MQ并不是python内置的模块，而是一个需要你额外安装（ubunto可直接apt-get其余请自行百度。）的程序，安装完毕后可通过python中内置的pika模块来调用MQ发送或接收队列请求。接下来我们就看几种python调用MQ的模式（作者自定义中文形象的模式名称）与方法。</p>
<p>参考：<a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000022193210">Python实现RabbitMQ中6种消息模型</a></p>
<h3 id="RabbitMQ消息模型"><a href="#RabbitMQ消息模型" class="headerlink" title="RabbitMQ消息模型"></a>RabbitMQ消息模型</h3><p> 这里使用<strong>Python</strong>的<strong>pika</strong>这个库来实现RabbitMQ中常见的6种消息模型。没有的可以先安装：</p>
<ol>
<li><p>单生产单消费模型：即完成基本的一对一消息转发。</p>
</li>
<li><p>消息分发模型：多个收听者监听一个队列。</p>
</li>
<li><p>fanout消息订阅模式：生产者将消息发送到Exchange，Exchange再转发到与之绑定的Queue中，每个消费者再到自己的Queue中取消息。</p>
</li>
<li><p>direct路由模式：此时生产者发送消息时需要指定RoutingKey，即路由Key，Exchange接收到消息时转发到与RoutingKey相匹配的队列中。</p>
</li>
<li><p>topic匹配模式：更细致的分组，允许在RoutingKey中使用匹配符。</p>
</li>
<li><p>RPC远程过程调用：客户端与服务器之间是完全解耦的，即两端既是消息的发送者也是接受者。</p>
</li>
</ol>
<h3 id="vhost"><a href="#vhost" class="headerlink" title="vhost"></a>vhost</h3><p>每一个 RabbitMQ 服务器都能创建 <strong>虚拟的消息服务器</strong>，称之为 <strong>虚拟主机（virtual host）</strong>，简称 <strong>vhost</strong>。</p>
<p>vhost 本质上是一个独立的小型 RabbitMQ 服务器，拥有自己独立的队列、交换器、绑定关系等，并且 <strong>拥有自己独立的权限</strong>。</p>
<p>vhost 可避免队列和交换器等命名冲突，<strong>vhost 之间是绝对隔离的</strong>，无法将 vhost1 中的交换器与 vhost2 中的队列进行绑定，这样的机制既保证了安全性，又确保可移植性。</p>
<h2 id="docker常用的命令"><a href="#docker常用的命令" class="headerlink" title="docker常用的命令"></a>docker常用的命令</h2><p>其他常用命令如下：<br>退出容器：exit<br>进入容器：docker start -i containerID<br>查看已有容器：docker ps -a<br>查看已有镜像：docker images<br>删除容器：docker rm containerID<br>删除镜像：docker rmi imageNAME<br>将当前容器保存为一个镜像：docker commit -a “yaohui” containerID imageNAME<br>将当前镜像保存为一个压缩文件：docker save -o name.tar imageNAME</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://xiehans.github.io/2022/05/10/docker%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E5%99%A8/" data-id="clgt0zr2u0008usu168iz3rs3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" rel="tag">服务器</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-AliyunPython" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/05/07/AliyunPython/" class="article-date">
  <time datetime="2022-05-07T08:24:12.000Z" itemprop="datePublished">2022-05-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/05/07/AliyunPython/">AliyunPython</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1、安装"><a href="#1、安装" class="headerlink" title="1、安装"></a>1、安装</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install aliyun-python-sdk-sts==3.1.0</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://xiehans.github.io/2022/05/07/AliyunPython/" data-id="clgt0zr2h0000usu166r3ccen" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-pytorch学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/05/05/pytorch%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2022-05-05T13:47:20.000Z" itemprop="datePublished">2022-05-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/05/05/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1、pytorch简介"><a href="#1、pytorch简介" class="headerlink" title="1、pytorch简介"></a>1、pytorch简介</h1><p>pytorch是一个能在cpu和gpu上运行并解决各类深度学习问题的深度框架，可以将其看做是支持GPU计算和自动微分计算的Numpy库。2017年1月 开源。2018年获得图灵奖。</p>
<p>pytorch具有自动求导的动态图功能，即define by run，即当python解释器运行到相应的行时才创建计算图。</p>
<p>优点：易于使用的API，支持python，非常类似于Numpy。</p>
<p>支持分布式训练，强大的生态系统</p>
<h2 id="1-1-pytorch安装"><a href="#1-1-pytorch安装" class="headerlink" title="1.1 pytorch安装"></a>1.1 pytorch安装</h2><p>pytorch1.9.0  2021.6发布</p>
<p>支持win7及以上，ubantu13.04，macOs 10.01及以上</p>
<p>3.5-3.9 64bit的python版本</p>
<h3 id="1-1-1-Win-平台："><a href="#1-1-1-Win-平台：" class="headerlink" title="1.1.1 Win 平台："></a>1.1.1 Win 平台：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio cpuonly -c pytorch</span><br></pre></td></tr></table></figure>

<p>没有-c pytorch 则走国内的镜像</p>
<h3 id="1-1-2GPU平台："><a href="#1-1-2GPU平台：" class="headerlink" title="1.1.2GPU平台："></a>1.1.2GPU平台：</h3><p>cuda是一种NVIDIA推出的通用并行计算框架，该框架使GPU能够解决复杂的计算问题。为了使用CUDA，需要安装cudatoolkit，可以和pytorch一并使用conda安装。</p>
<p>显示当前的显卡信息nivdia-smi</p>
<p>cuda Version：驱动支持的cuda最高版本，并不是装的cuda的版本</p>
<p>官网地址：<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></p>
<p>不是30系的显卡，选择cuda 10.2即可，使用下面命令安装</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio cudatoolkit=<span class="number">10.2</span> -c pytorch</span><br></pre></td></tr></table></figure>

<p>30系的显卡，比如：3060，3070，3080 必须安装cuda11.1以上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio cudatoolkit=<span class="number">11.1</span> -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure>

<p>测试安装好的torch</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">torch.__version__</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure>

<h1 id="2、pytorch基础"><a href="#2、pytorch基础" class="headerlink" title="2、pytorch基础"></a>2、pytorch基础</h1><h2 id="2-1-机器学习的基础"><a href="#2-1-机器学习的基础" class="headerlink" title="2.1 机器学习的基础"></a>2.1 机器学习的基础</h2><p>1、创建模型</p>
<p>2、输入一张带标签的图片</p>
<p>3、使用模型对此图片做出预测</p>
<p>4、将预测结果与实际标签比较，产生的差距为损失</p>
<p>5、以减小损失为优化目标，根据损失优化模型参数</p>
<h2 id="2-2-基础模型创建流程"><a href="#2-2-基础模型创建流程" class="headerlink" title="2.2 基础模型创建流程"></a>2.2 基础模型创建流程</h2><p>1、输入数据处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">&#x27;xxx.csv&#x27;</span>)</span><br><span class="line">data.head()<span class="comment">#查看头信息</span></span><br><span class="line">data.info()<span class="comment">#查看数据集</span></span><br><span class="line">X= data.Education.values <span class="comment">#ndarray数值，Education为列名字</span></span><br><span class="line">X = X.reshape(-<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line"><span class="comment">#转换为tensor数值,tensor是torch最基本的数据结构</span></span><br><span class="line">torch.from_numpy(X).<span class="built_in">type</span>(torch.FloatTensor) <span class="comment">#转换为float的tensor。</span></span><br></pre></td></tr></table></figure>

<p>2、创建模型</p>
<p>模型必须继承自 nn.Module</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn<span class="comment">#层和损失函数均在该模型中</span></span><br><span class="line">calss EIModel(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="built_in">super</span>(EIModel,self).__init__()</span><br><span class="line">		self.linear = nn.Linear(in_features=<span class="number">1</span>,out_features=<span class="number">1</span>)<span class="comment">#输入的特征长度</span></span><br><span class="line">	<span class="comment">#方法的使用,如何使用定义的方法和层,前向传播</span></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputs</span>):</span><br><span class="line">		logits = self.linear(inputs)</span><br><span class="line">		<span class="keyword">return</span> logits</span><br><span class="line">	</span><br></pre></td></tr></table></figure>

<p><code>in_features</code>指的是输入的二维张量的大小，即输入的<code>[batch_size, size]</code>中的size</p>
<p>out_features 指的是输出的二维张量的大小，即输出的二维张量的形状为<code>[batch_size，output_size]</code>，当然，它也代表了该全连接层的神经元个数。</p>
<p>从输入输出的张量的shape角度来理解，相当于一个输入为<code>[batch_size, in_features]</code>的张量变换成了<code>[batch_size, out_features]</code>的输出张量</p>
<p>3、初始化模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = EIModel()<span class="comment">#初始化模型</span></span><br><span class="line">loss_fn = nn.MSELoss() <span class="comment">#定义损失函数</span></span><br><span class="line">opt = torch.optim.SGD(model.parameters()，lr = <span class="number">0.001</span>)<span class="comment">#定义优化函数</span></span><br></pre></td></tr></table></figure>

<p>model.parameters()为可训练的参数，也即优化的参数，训练目标项</p>
<p>4、训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>):</span><br><span class="line">	<span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(X,Y):</span><br><span class="line">		y_pred = model(x)<span class="comment">#forward的方法</span></span><br><span class="line">		loss = loss_fn(y_pred,y)</span><br><span class="line">		opt.zero_grad()<span class="comment">#清空以前的梯度</span></span><br><span class="line">		loss.backward()<span class="comment">#反向传播</span></span><br><span class="line">		opt.step()<span class="comment">#沿着损失最快的方法下降</span></span><br></pre></td></tr></table></figure>

<p>5、预测、评价</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>(model.parameters())<span class="comment">#输出参数</span></span><br><span class="line"><span class="built_in">list</span>(model.named_parameters())<span class="comment">#输出带名字的参数</span></span><br><span class="line">model.linear.weight,model.linear.bias</span><br></pre></td></tr></table></figure>

<h1 id="3、Tensor-张量"><a href="#3、Tensor-张量" class="headerlink" title="3、Tensor(张量)"></a>3、Tensor(张量)</h1><p>Pytorch最基本的操作对象是Tensor(张量)，它表示一个多维矩阵，张量类似于Numpy的ndarrays，张量可以在GPU上使用以加速计算。</p>
<p>Tensor(张量)和 Scalar 标量；Vector 向量；Matrix 矩阵 一个等级</p>
<p>张量是基于向量和矩阵的推广，我们可以将标量视为零阶张量，向量视为一阶张量，矩阵就是二阶张量。张量支持高效科学计算的数组，它可以是一个数（标量）、一维数组(向量)、二维数组(矩阵)和更高维的数组(高阶数据)</p>
<h2 id="3-1-初始化张量"><a href="#3-1-初始化张量" class="headerlink" title="3.1 初始化张量"></a>3.1 初始化张量</h2><h3 id="3-1-1-从列表创建张量"><a href="#3-1-1-从列表创建张量" class="headerlink" title="3.1.1 从列表创建张量"></a>3.1.1 从列表创建张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])<span class="comment">#从列表创建张量</span></span><br><span class="line">t.dtype  <span class="comment">#tensor的属性，输出数据类型</span></span><br><span class="line"></span><br><span class="line">t = torch.FloatTensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])<span class="comment">#创建float类型的tensor,torch.float32</span></span><br><span class="line">t = torch.LongTensor([<span class="number">1</span>,<span class="number">2</span>])<span class="comment">#torch.int64</span></span><br></pre></td></tr></table></figure>

<h3 id="3-1-2-从ndarray创建"><a href="#3-1-2-从ndarray创建" class="headerlink" title="3.1.2 从ndarray创建"></a>3.1.2 从ndarray创建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np_array = np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">np_array.dtype</span><br><span class="line">t = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure>

<h3 id="3-1-3-Tensor的最基本数据类型"><a href="#3-1-3-Tensor的最基本数据类型" class="headerlink" title="3.1.3 Tensor的最基本数据类型"></a>3.1.3 Tensor的最基本数据类型</h3><ul>
<li>32位浮点型：torch.float32&#x2F;torch.float (数据经常用这个类型)</li>
<li>64位浮点型：torch.float64</li>
<li>64位整型：torch.int64&#x2F;torch.long (标签经常用这个类型)</li>
<li>16位整型：torch.int16</li>
<li>32位整型：torch.int32</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="comment">#等价于</span></span><br><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32)</span><br></pre></td></tr></table></figure>

<h2 id="3-2-张量的随机值"><a href="#3-2-张量的随机值" class="headerlink" title="3.2 张量的随机值"></a>3.2 张量的随机值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.rand(<span class="number">2</span>,<span class="number">3</span>)<span class="comment">#产生2行3列，0-1的数值</span></span><br><span class="line">t = torch.randn(<span class="number">2</span>,<span class="number">3</span>)<span class="comment">#标准正态分布的值，非0~1</span></span><br><span class="line">t.shape<span class="comment">#查看形状，torch.Size([2,3])</span></span><br><span class="line"></span><br><span class="line">torch.zeros(<span class="number">3</span>,<span class="number">4</span>)<span class="comment">#全0</span></span><br><span class="line">torch.ones(<span class="number">3</span>,<span class="number">2</span>)<span class="comment">#全1</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建形状、类型和原本tensor一样的tensor</span></span><br><span class="line">x = torch.zeros_like(t) <span class="comment">#全0</span></span><br><span class="line">x = torch.rand_like(t)	<span class="comment">#随机数</span></span><br><span class="line">t.size()</span><br><span class="line">t.size(<span class="number">0</span>)				<span class="comment">#0维的大小</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="3-3-tensor数据移动到显存"><a href="#3-3-tensor数据移动到显存" class="headerlink" title="3.3 tensor数据移动到显存"></a>3.3 tensor数据移动到显存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t.device						<span class="comment">#device(type=&#x27;cpu&#x27;)</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():	  <span class="comment">#转换到cuda上面</span></span><br><span class="line">	t = t.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">t.device						<span class="comment">#device(type=&#x27;cuda&#x27;,index=0)</span></span><br><span class="line">t = t.to(<span class="string">&#x27;cpu&#x27;</span>)					<span class="comment">#显存移动到内存</span></span><br></pre></td></tr></table></figure>

<h2 id="3-4-数据类型的转换"><a href="#3-4-数据类型的转换" class="headerlink" title="3.4 数据类型的转换"></a>3.4 数据类型的转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式一 不推荐使用</span></span><br><span class="line">t.dtype<span class="comment">#torch.float32</span></span><br><span class="line">t = t.<span class="built_in">type</span>(torch.float16)<span class="comment">#数据类型转换</span></span><br><span class="line">t = t.<span class="built_in">type</span>(torch.int16)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方式二 推荐使用</span></span><br><span class="line">t = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t.long()  <span class="comment">#等价于t.type(torch.int64)</span></span><br><span class="line">t.<span class="built_in">float</span>()	<span class="comment">#等价于t.type(torch.float32)</span></span><br></pre></td></tr></table></figure>

<h2 id="3-5-张量的运算及转换"><a href="#3-5-张量的运算及转换" class="headerlink" title="3.5 张量的运算及转换"></a>3.5 张量的运算及转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#广播模式</span></span><br><span class="line">t + <span class="number">3</span><span class="comment">#每个元素均加3</span></span><br><span class="line">t + t1	<span class="comment">#相同结构，对应元素相加</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t.add(t1)<span class="comment">#等价于t+t1，不改变原来的值</span></span><br><span class="line">t.add_(t1)<span class="comment">#覆盖掉t值，就地改变原值</span></span><br></pre></td></tr></table></figure>

<p>#其他常用函数,顶级方法<br>abs,mean,cumsum(累加和),multiply,divide(除法)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(t)<span class="comment">#相当于t.abs()</span></span><br><span class="line">torch.mean(t) <span class="comment">#相当于t.mean()</span></span><br></pre></td></tr></table></figure>

<p>矩阵乘法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># matmul 矩阵乘法</span></span><br><span class="line">t.T <span class="comment">#转置</span></span><br><span class="line">t.matmul(t.T)<span class="comment">#俩个矩阵乘法</span></span><br><span class="line"><span class="comment">#简写</span></span><br><span class="line">t@(t.T)  <span class="comment">#相当于t.matmul(t.T)</span></span><br></pre></td></tr></table></figure>

<p>item() 方法</p>
<p>tensor转化为python的数据类型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#维度为0的tensor数据转换为python的数据类型</span></span><br><span class="line">result = t.<span class="built_in">sum</span>().item()</span><br></pre></td></tr></table></figure>

<p>numpy和tensor的转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#numpy转换为tensor</span></span><br><span class="line">t1 = torch.from_numpy(np.random.randn(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">#tensor转换为numpy</span></span><br><span class="line">t1.numpy()</span><br></pre></td></tr></table></figure>

<h2 id="3-6-张量变形"><a href="#3-6-张量变形" class="headerlink" title="3.6  张量变形"></a>3.6  张量变形</h2><h3 id="3-6-1-t-view"><a href="#3-6-1-t-view" class="headerlink" title="3.6.1 t.view()"></a>3.6.1 t.view()</h3><p>#相当于np.reshape的方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">t.shape</span><br><span class="line">t1 = t.view(3,8)</span><br><span class="line">t1.shape</span><br><span class="line">t.view(-1,1)#-1表示自动计算，24*1的形状</span><br><span class="line">t = torch.randn(12,3,4,4)</span><br><span class="line">#四维数据变成二维</span><br><span class="line">t2 = t.view(12,3*4*4)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="3-6-2-torch-squeeze"><a href="#3-6-2-torch-squeeze" class="headerlink" title="3.6.2  torch.squeeze"></a>3.6.2  torch.squeeze</h3><p>减少为1的维度</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1 = t1.view(1,12,48)</span><br><span class="line">t2 = torch.squeeze(t1)</span><br></pre></td></tr></table></figure>

<h3 id="3-6-3-torch-unsqueeze"><a href="#3-6-3-torch-unsqueeze" class="headerlink" title="3.6.3 torch.unsqueeze"></a>3.6.3 torch.unsqueeze</h3><p>增加维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#两种方式实现增加维度</span></span><br><span class="line">input_y = torch.unsqueeze(input_y,<span class="number">2</span>)</span><br><span class="line">input_y = input_y.unsqueeze(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>



<h2 id="3-7-自动微分"><a href="#3-7-自动微分" class="headerlink" title="3.7 自动微分"></a>3.7 自动微分</h2><p>t.requires_grad是否需要框架跟随计算，true表示需要跟随</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(<span class="number">2</span>,<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = t+<span class="number">5</span><span class="comment"># y有一个grad_fn属性，返回计算得到此张量的方法</span></span><br><span class="line">y.requires_grad  <span class="comment">#True,会继承t</span></span><br><span class="line"></span><br><span class="line">z = y*<span class="number">2</span></span><br><span class="line">out = z.mean() <span class="comment">#标量值</span></span><br><span class="line"><span class="comment">#标量值可以调用自动微分运算</span></span><br><span class="line">out.backward()</span><br><span class="line">t.grad  <span class="comment">#打印t的梯度，d(out)/d(t)</span></span><br><span class="line"><span class="comment">#grad属性记录得到的梯度</span></span><br><span class="line"><span class="comment"># t没有gran_fn</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不计算梯度，方法一</span></span><br><span class="line"><span class="keyword">with</span> tirch.no.grad():</span><br><span class="line">    y = t=<span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(y.requires_grad)</span><br><span class="line"><span class="comment"># 方式二</span></span><br><span class="line">t.requires_grad(<span class="literal">False</span>)<span class="comment">#就地改变属性，#不再优化参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#截断梯度运算，方法三</span></span><br><span class="line">result = out.detach()<span class="comment">#detach之后的运算，不再跟踪运算</span></span><br></pre></td></tr></table></figure>

<p>Tensor :属性 data；grad；grad_fn</p>
<h1 id="4、nn-和nn-functional的区别"><a href="#4、nn-和nn-functional的区别" class="headerlink" title="4、nn 和nn.functional的区别"></a>4、nn 和nn.functional的区别</h1><h2 id="4-1-相同点"><a href="#4-1-相同点" class="headerlink" title="4.1 相同点"></a>4.1 相同点</h2><ul>
<li><code>nn.Xxx</code>和<code>nn.functional.xxx</code>的实际功能是相同的，即<code>nn.Conv2d</code>和<code>nn.functional.conv2d</code> 都是进行卷积，<code>nn.Dropout</code> 和<code>nn.functional.dropout</code>都是进行dropout 等； </li>
<li>运行效率也是近乎相同。</li>
</ul>
<p><code>nn.functional.xxx</code>是函数接口，而<code>nn.Xxx</code>是<code>nn.functional.xxx</code>的类封装，并且**<code>nn.Xxx</code>都继承于一个共同祖先<code>nn.Module</code>。**这一点导致<code>nn.Xxx</code>除了具有<code>nn.functional.xxx</code>功能之外，内部附带了<code>nn.Module</code>相关的属性和方法，例如<code>train(), eval(),load_state_dict, state_dict </code>等。</p>
<h2 id="4-2-差别点"><a href="#4-2-差别点" class="headerlink" title="4.2 差别点"></a>4.2 差别点</h2><ul>
<li><strong>两者的调用方式不同。</strong></li>
</ul>
<p>1）是否需要实例化</p>
<p><code>nn.Xxx</code> 需要先实例化并传入参数，然后以函数调用的方式调用实例化的对象并传入输入数据</p>
<p><code>nn.functional.xxx</code>同时传入输入数据和weight, bias等其他参数</p>
<p>2）使用</p>
<p><strong><code>nn.Xxx</code>继承于<code>nn.Module</code>， 能够很好的与<code>nn.Sequential</code>结合使用， 而<code>nn.functional.xxx</code>无法与<code>nn.Sequential</code>结合使用。</strong></p>
<p><strong><code>nn.Xxx</code>不需要你自己定义和管理weight；而<code>nn.functional.xxx</code>需要你自己定义weight，每次调用的时候都需要手动传入weight, 不利于代码复用。</strong></p>
<p>3）PyTorch官方推荐</p>
<p>具有学习参数的（例如，conv2d, linear, batch_norm)采用<code>nn.Xxx</code>方式，没有学习参数的（例如，maxpool, loss func, activation func）等推荐使用<code>nn.functional.xxx</code>或者<code>nn.Xxx</code>方式。</p>
<p>但关于dropout，个人强烈推荐使用<code>nn.Xxx</code>方式，因为一般情况下只有训练阶段才进行dropout，在eval阶段都不会进行dropout。使用<code>nn.Xxx</code>方式定义dropout，在调用<code>model.eval()</code>之后，model中所有的dropout layer都关闭，但以<code>nn.function.dropout</code>方式定义dropout，在调用<code>model.eval()</code>之后并不能关闭dropout。</p>
<p>最后，总结如下：</p>
<p>这个问题依赖于你要解决的问题复杂度和个人风格喜好。在<code>nn.Xxx</code>不能满足你的功能需求时，<code>nn.functional.xxx</code>是更佳的选择，因为<code>nn.functional.xxx</code>更加的灵活(更加接近底层），你可以在其基础上定义出自己想要的功能。 个人偏向于在能使用<code>nn.Xxx</code>情况下尽量使用，不行再换<code>nn.functional.xxx</code> ，感觉这样更能显示出网络的层次关系，也更加的纯粹（所有layer和model本身都是Module，一种和谐统一的感觉）。</p>
<h1 id="5、卷积的尺寸"><a href="#5、卷积的尺寸" class="headerlink" title="5、卷积的尺寸"></a>5、卷积的尺寸</h1><p>输入的尺寸需要为(数据量* 维度 * 数据长度)</p>
<p>self.conv1d &#x3D; nn.Conv1d(in_channels&#x3D;1, out_channels&#x3D;16, kernel_size&#x3D;1, stride&#x3D;1, padding&#x3D;5, dilation&#x3D;1);</p>
<p>中间数字为 维度</p>
<h1 id="6、数据集"><a href="#6、数据集" class="headerlink" title="6、数据集"></a>6、数据集</h1><h2 id="6-1、加载内置数据集"><a href="#6-1、加载内置数据集" class="headerlink" title="6.1、加载内置数据集"></a>6.1、加载内置数据集</h2><p>torchvision库，提供了常用的数据集，模型和转换函数等。</p>
<p>pytorch提供了两个类</p>
<p>torch.utils.data.Dataset 类</p>
<p>torch.utils.data.DataLoader 类</p>
<p>pytorch内置图片数据集均在torchvision.datasets模块下，包含Caltrech，CelebA，CIFAR，Cityscapes、COCO、Fashion-MNIST、ImageNet、MINIST等很多著名的数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvison</span><br><span class="line"><span class="keyword">from</span> torchvison.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line">train_ds = torchvision.datasets.MINIST(<span class="string">&#x27;./&#x27;</span>,					<span class="comment">#路径</span></span><br><span class="line">                                       train=<span class="literal">True</span>,	 			 <span class="comment">#下载训练数据</span></span><br><span class="line">                          			  trainsform = ToTensor(),	 <span class="comment">#转换为tensor</span></span><br><span class="line">                           			  download=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>ToTensor() 作用：</p>
<ul>
<li>1、转为Tensor</li>
<li>2、规范图片格式。常见图片格式：(高，宽，通道)，框架默认处理的格式为(channel,height,weight)</li>
<li>3、将像素值规范到0-1之间。</li>
</ul>
<h2 id="6-2-创建dataLoader"><a href="#6-2-创建dataLoader" class="headerlink" title="6.2 创建dataLoader"></a>6.2 创建dataLoader</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.utils.data.DataLoader <span class="comment">#对dataset进行封装</span></span><br></pre></td></tr></table></figure>

<p>封装主要做三件事情：</p>
<ul>
<li>1、乱序 shuffle</li>
<li>2、数据采样为小批次 batch_size 指定批次的大小</li>
<li>3、num_workers 多进程读取</li>
<li>4、clooate_fn 设置批次处理函数,可以自定义</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练数据需要shuffle，测试数据不需要乱序</span></span><br><span class="line">train_dl = torch.utils.data.DataLoader(train_ds,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 取出一个批次的数据</span></span><br><span class="line">data,labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dl))</span><br><span class="line">data.shape</span><br><span class="line">(size,channel,height,weight)</span><br></pre></td></tr></table></figure>

<p>绘图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i , img <span class="keyword">in</span> <span class="built_in">enumerate</span>(imgs[:<span class="number">10</span>]):</span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    npimg = np.aqueeze(npimg)</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">10</span>,i+<span class="number">1</span>)<span class="comment">#从i+1开始编码</span></span><br><span class="line">    plt.imshow(npimg)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">labels[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<h1 id="7、保存和加载pytorch模型"><a href="#7、保存和加载pytorch模型" class="headerlink" title="7、保存和加载pytorch模型"></a>7、保存和加载pytorch模型</h1><h2 id="7-1-保存多个模型到一个文件"><a href="#7-1-保存多个模型到一个文件" class="headerlink" title="7.1 保存多个模型到一个文件"></a>7.1 保存多个模型到一个文件</h2><p>模型保存</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line">            <span class="string">&#x27;modelA_state_dict&#x27;</span>: modelA.state_dict(),</span><br><span class="line">            <span class="string">&#x27;modelB_state_dict&#x27;</span>: modelB.state_dict(),</span><br><span class="line">            <span class="string">&#x27;optimizerA_state_dict&#x27;</span>: optimizerA.state_dict(),</span><br><span class="line">            <span class="string">&#x27;optimizerB_state_dict&#x27;</span>: optimizerB.state_dict(),</span><br><span class="line">            ...</span><br><span class="line">            &#125;, PATH)</span><br></pre></td></tr></table></figure>

<p>模型加载</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">modelA = TheModelAClass(*args, **kwargs)<span class="comment">#模型结构</span></span><br><span class="line">modelB = TheModelBClass(*args, **kwargs)<span class="comment">#模型结构</span></span><br><span class="line">optimizerA = TheOptimizerAClass(*args, **kwargs)<span class="comment">#模型结构</span></span><br><span class="line">optimizerB = TheOptimizerBClass(*args, **kwargs)<span class="comment">#模型结构</span></span><br><span class="line"> </span><br><span class="line">checkpoint = torch.load(PATH)<span class="comment">#模型保存的路径</span></span><br><span class="line">modelA.load_state_dict(checkpoint[<span class="string">&#x27;modelA_state_dict&#x27;</span>])<span class="comment">#对应的权重</span></span><br><span class="line">modelB.load_state_dict(checkpoint[<span class="string">&#x27;modelB_state_dict&#x27;</span>])</span><br><span class="line">optimizerA.load_state_dict(checkpoint[<span class="string">&#x27;optimizerA_state_dict&#x27;</span>])</span><br><span class="line">optimizerB.load_state_dict(checkpoint[<span class="string">&#x27;optimizerB_state_dict&#x27;</span>])</span><br><span class="line"> </span><br><span class="line">modelA.<span class="built_in">eval</span>()</span><br><span class="line">modelB.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># - 或者 -</span></span><br><span class="line">modelA.train()</span><br><span class="line">modelB.train()</span><br></pre></td></tr></table></figure>

<h2 id="7-2-保存加载用于推理的常规Checkpoint-x2F-或继续训练"><a href="#7-2-保存加载用于推理的常规Checkpoint-x2F-或继续训练" class="headerlink" title="7.2 保存加载用于推理的常规Checkpoint&#x2F;或继续训练"></a>7.2 保存加载用于推理的常规Checkpoint&#x2F;或继续训练</h2><p>模型保存</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line">            &#x27;epoch&#x27;: epoch,</span><br><span class="line">            &#x27;model_state_dict&#x27;: model.state_dict(),</span><br><span class="line">            &#x27;optimizer_state_dict&#x27;: optimizer.state_dict(),</span><br><span class="line">            &#x27;loss&#x27;: loss,</span><br><span class="line">            ...</span><br><span class="line">            &#125;, PATH)</span><br></pre></td></tr></table></figure>

<p>数据加载</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">optimizer = TheOptimizerClass(*args, **kwargs)</span><br><span class="line"> </span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[&#x27;model_state_dict&#x27;])</span><br><span class="line">optimizer.load_state_dict(checkpoint[&#x27;optimizer_state_dict&#x27;])</span><br><span class="line">epoch = checkpoint[&#x27;epoch&#x27;]</span><br><span class="line">loss = checkpoint[&#x27;loss&#x27;]</span><br><span class="line"> </span><br><span class="line">model.eval()</span><br><span class="line"># - 或者 -</span><br><span class="line">model.train()</span><br></pre></td></tr></table></figure>

<p>8、激活函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">nn.LeakyReLU(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>

<p>激活函数的特点：线性可导的</p>
<h1 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h1><h2 id="1、UnicodeDecodeError"><a href="#1、UnicodeDecodeError" class="headerlink" title="1、UnicodeDecodeError"></a>1、UnicodeDecodeError</h2><p>Python 报错：UnicodeDecodeError: ‘gbk’ codec can’t decode byte 0x80 in position 0: illegal multibyte sequence</p>
<p>问题分析及解决办法： 序列化操作时，文件模式不正确，改为“rb+”</p>
<h2 id="2、dir-函数"><a href="#2、dir-函数" class="headerlink" title="2、dir ()函数"></a>2、dir ()函数</h2><p>查看对象内的所有的属性和方法</p>
<h2 id="3、PyTroch报错NotImplementedError"><a href="#3、PyTroch报错NotImplementedError" class="headerlink" title="3、PyTroch报错NotImplementedError:"></a>3、PyTroch报错NotImplementedError:</h2><p>‘forward’定义处<strong>缩进错误</strong>！</p>
<h2 id="4、Pytorch全局平均池化"><a href="#4、Pytorch全局平均池化" class="headerlink" title="4、Pytorch全局平均池化"></a>4、Pytorch全局平均池化</h2><p>没有对全局平均（最大）池化单独封装为一层。需要自己实现</p>
<h2 id="5、Linear"><a href="#5、Linear" class="headerlink" title="5、Linear"></a>5、Linear</h2><p>shape（N, *, H）值得注意的是input的形状和torch的CNN输入形状稍有不同</p>
<p>CNN的输入形状为 数量*通道数 * 特征长度</p>
<p>Linear 的输入形状为 数量 * 特征长度 * 通道数</p>
<p><img src="/pytorch%E5%AD%A6%E4%B9%A0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMjYzNDc3,size_16,color_FFFFFF,t_70.png" alt="img"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://xiehans.github.io/2022/05/05/pytorch%E5%AD%A6%E4%B9%A0/" data-id="clgt0zr30000husu1g9bb90dd" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B7%A5%E7%A8%8B%E7%AC%94%E8%AE%B0/" rel="tag">工程笔记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" rel="tag">服务器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A1%AC%E4%BB%B6/" rel="tag">硬件</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/%E5%B7%A5%E7%A8%8B%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">工程笔记</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 20px;">服务器</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">深度学习</a> <a href="/tags/%E7%A1%AC%E4%BB%B6/" style="font-size: 10px;">硬件</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 10px;">算法</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/04/21/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/04/21/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2022/05/26/MP%E7%AE%97%E6%B3%95%E8%A7%A3%E9%87%8A/">MP算法解释</a>
          </li>
        
          <li>
            <a href="/2022/05/26/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">git常用命令</a>
          </li>
        
          <li>
            <a href="/2022/05/26/%E4%B8%89%E6%AD%A5%E5%AD%A6%E4%BC%9Apytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/">三步学会pytorch搭建深度模型</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>